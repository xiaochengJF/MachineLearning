{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GBDT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1dJPhOIqqwQlNUa0AilBqHXYPfGot0CIj",
      "authorship_tag": "ABX9TyMroefeh/X2p7X+YPe9hKZI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xiaochengJF/MachineLearning/blob/master/GBDT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WQzysfW2S3q",
        "colab_type": "text"
      },
      "source": [
        "# GBDT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mePjpcdE2W7i",
        "colab_type": "text"
      },
      "source": [
        "## 工具\n",
        "### 基本函数库"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTGTsX4KUIMM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import division\n",
        "from itertools import combinations_with_replacement\n",
        "import numpy as np\n",
        "import math\n",
        "import sys\n",
        "\n",
        "\n",
        "def shuffle_data(X, y, seed=None):\n",
        "    \"\"\" Random shuffle of the samples in X and y \"\"\"\n",
        "    if seed:\n",
        "        np.random.seed(seed)\n",
        "    idx = np.arange(X.shape[0])\n",
        "    np.random.shuffle(idx)\n",
        "    return X[idx], y[idx]\n",
        "\n",
        "\n",
        "def batch_iterator(X, y=None, batch_size=64):\n",
        "    \"\"\" Simple batch generator \"\"\"\n",
        "    n_samples = X.shape[0]\n",
        "    for i in np.arange(0, n_samples, batch_size):\n",
        "        begin, end = i, min(i+batch_size, n_samples)\n",
        "        if y is not None:\n",
        "            yield X[begin:end], y[begin:end]\n",
        "        else:\n",
        "            yield X[begin:end]\n",
        "\n",
        "\n",
        "def divide_on_feature(X, feature_i, threshold):\n",
        "    \"\"\" Divide dataset based on if sample value on feature index is larger than\n",
        "        the given threshold \"\"\"\n",
        "    split_func = None\n",
        "    if isinstance(threshold, int) or isinstance(threshold, float):\n",
        "        split_func = lambda sample: sample[feature_i] >= threshold\n",
        "    else:\n",
        "        split_func = lambda sample: sample[feature_i] == threshold\n",
        "\n",
        "    X_1 = np.array([sample for sample in X if split_func(sample)])\n",
        "    X_2 = np.array([sample for sample in X if not split_func(sample)])\n",
        "\n",
        "    return np.array([X_1, X_2])\n",
        "\n",
        "\n",
        "def polynomial_features(X, degree):\n",
        "    n_samples, n_features = np.shape(X)\n",
        "\n",
        "    def index_combinations():\n",
        "        combs = [combinations_with_replacement(range(n_features), i) for i in range(0, degree + 1)]\n",
        "        flat_combs = [item for sublist in combs for item in sublist]\n",
        "        return flat_combs\n",
        "    \n",
        "    combinations = index_combinations()\n",
        "    n_output_features = len(combinations)\n",
        "    X_new = np.empty((n_samples, n_output_features))\n",
        "    \n",
        "    for i, index_combs in enumerate(combinations):  \n",
        "        X_new[:, i] = np.prod(X[:, index_combs], axis=1)\n",
        "\n",
        "    return X_new\n",
        "\n",
        "\n",
        "def get_random_subsets(X, y, n_subsets, replacements=True):\n",
        "    \"\"\" Return random subsets (with replacements) of the data \"\"\"\n",
        "    n_samples = np.shape(X)[0]\n",
        "    # Concatenate x and y and do a random shuffle\n",
        "    X_y = np.concatenate((X, y.reshape((1, len(y))).T), axis=1)\n",
        "    np.random.shuffle(X_y)\n",
        "    subsets = []\n",
        "\n",
        "    # Uses 50% of training samples without replacements\n",
        "    subsample_size = int(n_samples // 2)\n",
        "    if replacements:\n",
        "        subsample_size = n_samples      # 100% with replacements\n",
        "\n",
        "    for _ in range(n_subsets):\n",
        "        idx = np.random.choice(\n",
        "            range(n_samples),\n",
        "            size=np.shape(range(subsample_size)),\n",
        "            replace=replacements)\n",
        "        X = X_y[idx][:, :-1]\n",
        "        y = X_y[idx][:, -1]\n",
        "        subsets.append([X, y])\n",
        "    return subsets\n",
        "\n",
        "\n",
        "def normalize(X, axis=-1, order=2):\n",
        "    \"\"\" Normalize the dataset X \"\"\"\n",
        "    l2 = np.atleast_1d(np.linalg.norm(X, order, axis))\n",
        "    l2[l2 == 0] = 1\n",
        "    return X / np.expand_dims(l2, axis)\n",
        "\n",
        "\n",
        "def standardize(X):\n",
        "    \"\"\" Standardize the dataset X \"\"\"\n",
        "    X_std = X\n",
        "    mean = X.mean(axis=0)\n",
        "    std = X.std(axis=0)\n",
        "    for col in range(np.shape(X)[1]):\n",
        "        if std[col]:\n",
        "            X_std[:, col] = (X_std[:, col] - mean[col]) / std[col]\n",
        "    # X_std = (X - X.mean(axis=0)) / X.std(axis=0)\n",
        "    return X_std\n",
        "\n",
        "\n",
        "def train_test_split(X, y, test_size=0.5, shuffle=True, seed=None):\n",
        "    \"\"\" Split the data into train and test sets \"\"\"\n",
        "    if shuffle:\n",
        "        X, y = shuffle_data(X, y, seed)\n",
        "    # Split the training data from test data in the ratio specified in\n",
        "    # test_size\n",
        "    split_i = len(y) - int(len(y) // (1 / test_size))\n",
        "    X_train, X_test = X[:split_i], X[split_i:]\n",
        "    y_train, y_test = y[:split_i], y[split_i:]\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "\n",
        "def k_fold_cross_validation_sets(X, y, k, shuffle=True):\n",
        "    \"\"\" Split the data into k sets of training / test data \"\"\"\n",
        "    if shuffle:\n",
        "        X, y = shuffle_data(X, y)\n",
        "\n",
        "    n_samples = len(y)\n",
        "    left_overs = {}\n",
        "    n_left_overs = (n_samples % k)\n",
        "    if n_left_overs != 0:\n",
        "        left_overs[\"X\"] = X[-n_left_overs:]\n",
        "        left_overs[\"y\"] = y[-n_left_overs:]\n",
        "        X = X[:-n_left_overs]\n",
        "        y = y[:-n_left_overs]\n",
        "\n",
        "    X_split = np.split(X, k)\n",
        "    y_split = np.split(y, k)\n",
        "    sets = []\n",
        "    for i in range(k):\n",
        "        X_test, y_test = X_split[i], y_split[i]\n",
        "        X_train = np.concatenate(X_split[:i] + X_split[i + 1:], axis=0)\n",
        "        y_train = np.concatenate(y_split[:i] + y_split[i + 1:], axis=0)\n",
        "        sets.append([X_train, X_test, y_train, y_test])\n",
        "\n",
        "    # Add left over samples to last set as training samples\n",
        "    if n_left_overs != 0:\n",
        "        np.append(sets[-1][0], left_overs[\"X\"], axis=0)\n",
        "        np.append(sets[-1][2], left_overs[\"y\"], axis=0)\n",
        "\n",
        "    return np.array(sets)\n",
        "\n",
        "\n",
        "def to_categorical(x, n_col=None):\n",
        "    \"\"\" One-hot encoding of nominal values \"\"\"\n",
        "    if not n_col:\n",
        "        n_col = np.amax(x) + 1\n",
        "    one_hot = np.zeros((x.shape[0], n_col))\n",
        "    one_hot[np.arange(x.shape[0]), x] = 1\n",
        "    return one_hot\n",
        "\n",
        "\n",
        "def to_nominal(x):\n",
        "    \"\"\" Conversion from one-hot encoding to nominal \"\"\"\n",
        "    return np.argmax(x, axis=1)\n",
        "\n",
        "\n",
        "def make_diagonal(x):\n",
        "    \"\"\" Converts a vector into an diagonal matrix \"\"\"\n",
        "    m = np.zeros((len(x), len(x)))\n",
        "    for i in range(len(m[0])):\n",
        "        m[i, i] = x[i]\n",
        "    return m"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzbV9Fp2UIIk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import division\n",
        "import numpy as np\n",
        "import math\n",
        "import sys\n",
        "\n",
        "\n",
        "def calculate_entropy(y):\n",
        "    \"\"\" Calculate the entropy of label array y \"\"\"\n",
        "    log2 = lambda x: math.log(x) / math.log(2)\n",
        "    unique_labels = np.unique(y)\n",
        "    entropy = 0\n",
        "    for label in unique_labels:\n",
        "        count = len(y[y == label])\n",
        "        p = count / len(y)\n",
        "        entropy += -p * log2(p)\n",
        "    return entropy\n",
        "\n",
        "\n",
        "def mean_squared_error(y_true, y_pred):\n",
        "    \"\"\" Returns the mean squared error between y_true and y_pred \"\"\"\n",
        "    mse = np.mean(np.power(y_true - y_pred, 2))\n",
        "    return mse\n",
        "\n",
        "\n",
        "def calculate_variance(X):\n",
        "    \"\"\" Return the variance of the features in dataset X \"\"\"\n",
        "    mean = np.ones(np.shape(X)) * X.mean(0)\n",
        "    n_samples = np.shape(X)[0]\n",
        "    variance = (1 / n_samples) * np.diag((X - mean).T.dot(X - mean))\n",
        "    \n",
        "    return variance\n",
        "\n",
        "\n",
        "def calculate_std_dev(X):\n",
        "    \"\"\" Calculate the standard deviations of the features in dataset X \"\"\"\n",
        "    std_dev = np.sqrt(calculate_variance(X))\n",
        "    return std_dev\n",
        "\n",
        "\n",
        "def euclidean_distance(x1, x2):\n",
        "    \"\"\" Calculates the l2 distance between two vectors \"\"\"\n",
        "    distance = 0\n",
        "    # Squared distance between each coordinate\n",
        "    for i in range(len(x1)):\n",
        "        distance += pow((x1[i] - x2[i]), 2)\n",
        "    return math.sqrt(distance)\n",
        "\n",
        "\n",
        "def accuracy_score(y_true, y_pred):\n",
        "    \"\"\" Compare y_true to y_pred and return the accuracy \"\"\"\n",
        "    accuracy = np.sum(y_true == y_pred, axis=0) / len(y_true)\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "def calculate_covariance_matrix(X, Y=None):\n",
        "    \"\"\" Calculate the covariance matrix for the dataset X \"\"\"\n",
        "    if Y is None:\n",
        "        Y = X\n",
        "    n_samples = np.shape(X)[0]\n",
        "    covariance_matrix = (1 / (n_samples-1)) * (X - X.mean(axis=0)).T.dot(Y - Y.mean(axis=0))\n",
        "\n",
        "    return np.array(covariance_matrix, dtype=float)\n",
        " \n",
        "\n",
        "def calculate_correlation_matrix(X, Y=None):\n",
        "    \"\"\" Calculate the correlation matrix for the dataset X \"\"\"\n",
        "    if Y is None:\n",
        "        Y = X\n",
        "    n_samples = np.shape(X)[0]\n",
        "    covariance = (1 / n_samples) * (X - X.mean(0)).T.dot(Y - Y.mean(0))\n",
        "    std_dev_X = np.expand_dims(calculate_std_dev(X), 1)\n",
        "    std_dev_y = np.expand_dims(calculate_std_dev(Y), 1)\n",
        "    correlation_matrix = np.divide(covariance, std_dev_X.dot(std_dev_y.T))\n",
        "\n",
        "    return np.array(correlation_matrix, dtype=float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6X5xfl6DUe86",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import division\n",
        "import numpy as np\n",
        "# from utils import accuracy_score\n",
        "\n",
        "class Loss(object):\n",
        "    def loss(self, y_true, y_pred):\n",
        "        return NotImplementedError()\n",
        "\n",
        "    def gradient(self, y, y_pred):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def acc(self, y, y_pred):\n",
        "        return 0\n",
        "\n",
        "class SquareLoss(Loss):\n",
        "    def __init__(self): pass\n",
        "\n",
        "    def loss(self, y, y_pred):\n",
        "        return 0.5 * np.power((y - y_pred), 2)\n",
        "\n",
        "    def gradient(self, y, y_pred):\n",
        "        return -(y - y_pred)\n",
        "\n",
        "class CrossEntropy(Loss):\n",
        "    def __init__(self): pass\n",
        "\n",
        "    def loss(self, y, p):\n",
        "        # Avoid division by zero\n",
        "        p = np.clip(p, 1e-15, 1 - 1e-15)\n",
        "        return - y * np.log(p) - (1 - y) * np.log(1 - p)\n",
        "\n",
        "    def acc(self, y, p):\n",
        "        return accuracy_score(np.argmax(y, axis=1), np.argmax(p, axis=1))\n",
        "\n",
        "    def gradient(self, y, p):\n",
        "        # Avoid division by zero\n",
        "        p = np.clip(p, 1e-15, 1 - 1e-15)\n",
        "        return - (y / p) + (1 - y) / (1 - p)\n",
        "\n",
        "\n",
        "class SotfMaxLoss(Loss):\n",
        "    def gradient(self, y, p):\n",
        "        return y - p"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nP97wRMP2gYR",
        "colab_type": "text"
      },
      "source": [
        "### 绘图工具"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xL4m9ijgVxQt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import progressbar\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cmx\n",
        "import matplotlib.colors as colors\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "bar_widgets = [\n",
        "    'Training: ', progressbar.Percentage(), ' ', progressbar.Bar(marker=\"-\", left=\"[\", right=\"]\"),\n",
        "    ' ', progressbar.ETA()\n",
        "]\n",
        "\n",
        "class Plot():\n",
        "    def __init__(self): \n",
        "        self.cmap = plt.get_cmap('viridis')\n",
        "\n",
        "    def _transform(self, X, dim):\n",
        "        covariance = calculate_covariance_matrix(X)\n",
        "        eigenvalues, eigenvectors = np.linalg.eig(covariance)\n",
        "        # Sort eigenvalues and eigenvector by largest eigenvalues\n",
        "        idx = eigenvalues.argsort()[::-1]\n",
        "        eigenvalues = eigenvalues[idx][:dim]\n",
        "        eigenvectors = np.atleast_1d(eigenvectors[:, idx])[:, :dim]\n",
        "        # Project the data onto principal components\n",
        "        X_transformed = X.dot(eigenvectors)\n",
        "\n",
        "        return X_transformed\n",
        "\n",
        "\n",
        "    def plot_regression(self, lines, title, axis_labels=None, mse=None, scatter=None, legend={\"type\": \"lines\", \"loc\": \"lower right\"}):\n",
        "        \n",
        "        if scatter:\n",
        "            scatter_plots = scatter_labels = []\n",
        "            for s in scatter:\n",
        "                scatter_plots += [plt.scatter(s[\"x\"], s[\"y\"], color=s[\"color\"], s=s[\"size\"])]\n",
        "                scatter_labels += [s[\"label\"]]\n",
        "            scatter_plots = tuple(scatter_plots)\n",
        "            scatter_labels = tuple(scatter_labels)\n",
        "\n",
        "        for l in lines:\n",
        "            li = plt.plot(l[\"x\"], l[\"y\"], color=s[\"color\"], linewidth=l[\"width\"], label=l[\"label\"])\n",
        "\n",
        "        if mse:\n",
        "            plt.suptitle(title)\n",
        "            plt.title(\"MSE: %.2f\" % mse, fontsize=10)\n",
        "        else:\n",
        "            plt.title(title)\n",
        "\n",
        "        if axis_labels:\n",
        "            plt.xlabel(axis_labels[\"x\"])\n",
        "            plt.ylabel(axis_labels[\"y\"])\n",
        "\n",
        "        if legend[\"type\"] == \"lines\":\n",
        "            plt.legend(loc=\"lower_left\")\n",
        "        elif legend[\"type\"] == \"scatter\" and scatter:\n",
        "            plt.legend(scatter_plots, scatter_labels, loc=legend[\"loc\"])\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    # Plot the dataset X and the corresponding labels y in 2D using PCA.\n",
        "    def plot_in_2d(self, X, y=None, title=None, accuracy=None, legend_labels=None):\n",
        "        X_transformed = self._transform(X, dim=2)\n",
        "        x1 = X_transformed[:, 0]\n",
        "        x2 = X_transformed[:, 1]\n",
        "        class_distr = []\n",
        "\n",
        "        y = np.array(y).astype(int)\n",
        "\n",
        "        colors = [self.cmap(i) for i in np.linspace(0, 1, len(np.unique(y)))]\n",
        "\n",
        "        # Plot the different class distributions\n",
        "        for i, l in enumerate(np.unique(y)):\n",
        "            _x1 = x1[y == l]\n",
        "            _x2 = x2[y == l]\n",
        "            _y = y[y == l]\n",
        "            class_distr.append(plt.scatter(_x1, _x2, color=colors[i]))\n",
        "\n",
        "        # Plot legend\n",
        "        if not legend_labels is None: \n",
        "            plt.legend(class_distr, legend_labels, loc=1)\n",
        "\n",
        "        # Plot title\n",
        "        if title:\n",
        "            if accuracy:\n",
        "                perc = 100 * accuracy\n",
        "                plt.suptitle(title)\n",
        "                plt.title(\"Accuracy: %.1f%%\" % perc, fontsize=10)\n",
        "            else:\n",
        "                plt.title(title)\n",
        "\n",
        "        # Axis labels\n",
        "        plt.xlabel('Principal Component 1')\n",
        "        plt.ylabel('Principal Component 2')\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    # Plot the dataset X and the corresponding labels y in 3D using PCA.\n",
        "    def plot_in_3d(self, X, y=None):\n",
        "        X_transformed = self._transform(X, dim=3)\n",
        "        x1 = X_transformed[:, 0]\n",
        "        x2 = X_transformed[:, 1]\n",
        "        x3 = X_transformed[:, 2]\n",
        "        fig = plt.figure()\n",
        "        ax = fig.add_subplot(111, projection='3d')\n",
        "        ax.scatter(x1, x2, x3, c=y)\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPng1OfY2s5U",
        "colab_type": "text"
      },
      "source": [
        "## 决策树"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0Sw_r1zVFjP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import division, print_function\n",
        "import numpy as np\n",
        "\n",
        "#from utils import divide_on_feature, train_test_split, standardize, mean_squared_error\n",
        "#from utils import calculate_entropy, accuracy_score, calculate_variance\n",
        "\n",
        "\n",
        "class DecisionNode():\n",
        "    \"\"\"Class that represents a decision node or leaf in the decision tree\n",
        "    Parameters:\n",
        "    -----------\n",
        "    feature_i: int\n",
        "        Feature index which we want to use as the threshold measure.\n",
        "    threshold: float\n",
        "        The value that we will compare feature values at feature_i against to\n",
        "        determine the prediction.\n",
        "    value: float\n",
        "        The class prediction if classification tree, or float value if regression tree.\n",
        "    true_branch: DecisionNode\n",
        "        Next decision node for samples where features value met the threshold.\n",
        "    false_branch: DecisionNode\n",
        "        Next decision node for samples where features value did not meet the threshold.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, feature_i=None, threshold=None,\n",
        "                 value=None, true_branch=None, false_branch=None):\n",
        "        self.feature_i = feature_i  # Index for the feature that is tested\n",
        "        self.threshold = threshold  # Threshold value for feature\n",
        "        self.value = value  # Value if the node is a leaf in the tree\n",
        "        self.true_branch = true_branch  # 'Left' subtree\n",
        "        self.false_branch = false_branch  # 'Right' subtree\n",
        "\n",
        "\n",
        "# Super class of RegressionTree and ClassificationTree\n",
        "class DecisionTree(object):\n",
        "    \"\"\"Super class of RegressionTree and ClassificationTree.\n",
        "    Parameters:\n",
        "    -----------\n",
        "    min_samples_split: int\n",
        "        The minimum number of samples needed to make a split when building a tree.\n",
        "    min_impurity: float\n",
        "        The minimum impurity required to split the tree further.\n",
        "    max_depth: int\n",
        "        The maximum depth of a tree.\n",
        "    loss: function\n",
        "        Loss function that is used for Gradient Boosting models to calculate impurity.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, min_samples_split=2, min_impurity=1e-7,\n",
        "                 max_depth=float(\"inf\"), loss=None):\n",
        "        self.root = None  # Root node in dec. tree\n",
        "        # Minimum n of samples to justify split\n",
        "        self.min_samples_split = min_samples_split\n",
        "        # The minimum impurity to justify split\n",
        "        self.min_impurity = min_impurity\n",
        "        # The maximum depth to grow the tree to\n",
        "        self.max_depth = max_depth\n",
        "        # Function to calculate impurity (classif.=>info gain, regr=>variance reduct.)\n",
        "        # 切割树的方法，gini，方差等\n",
        "        self._impurity_calculation = None\n",
        "        # Function to determine prediction of y at leaf\n",
        "        # 树节点取值的方法，分类树：选取出现最多次数的值，回归树：取所有值的平均值\n",
        "        self._leaf_value_calculation = None\n",
        "        # If y is one-hot encoded (multi-dim) or not (one-dim)\n",
        "        self.one_dim = None\n",
        "        # If Gradient Boost\n",
        "        self.loss = loss\n",
        "\n",
        "    def fit(self, X, y, loss=None):\n",
        "        \"\"\" Build decision tree \"\"\"\n",
        "        self.one_dim = len(np.shape(y)) == 1\n",
        "        self.root = self._build_tree(X, y)\n",
        "        self.loss = None\n",
        "\n",
        "    def _build_tree(self, X, y, current_depth=0):\n",
        "        \"\"\" Recursive method which builds out the decision tree and splits X and respective y\n",
        "        on the feature of X which (based on impurity) best separates the data\"\"\"\n",
        "        largest_impurity = 0\n",
        "        best_criteria = None  # Feature index and threshold\n",
        "        best_sets = None  # Subsets of the data\n",
        "\n",
        "        # Check if expansion of y is needed\n",
        "        if len(np.shape(y)) == 1:\n",
        "            y = np.expand_dims(y, axis=1)\n",
        "\n",
        "        # Add y as last column of X\n",
        "        Xy = np.concatenate((X, y), axis=1)\n",
        "\n",
        "        n_samples, n_features = np.shape(X)\n",
        "\n",
        "        if n_samples >= self.min_samples_split and current_depth <= self.max_depth:\n",
        "            # Calculate the impurity for each feature\n",
        "            for feature_i in range(n_features):\n",
        "                # All values of feature_i\n",
        "                feature_values = np.expand_dims(X[:, feature_i], axis=1)\n",
        "                unique_values = np.unique(feature_values)\n",
        "\n",
        "                # Iterate through all unique values of feature column i and\n",
        "                # calculate the impurity\n",
        "                for threshold in unique_values:\n",
        "                    # Divide X and y depending on if the feature value of X at index feature_i\n",
        "                    # meets the threshold\n",
        "                    Xy1, Xy2 = divide_on_feature(Xy, feature_i, threshold)\n",
        "\n",
        "                    if len(Xy1) > 0 and len(Xy2) > 0:\n",
        "                        # Select the y-values of the two sets\n",
        "                        y1 = Xy1[:, n_features:]\n",
        "                        y2 = Xy2[:, n_features:]\n",
        "\n",
        "                        # Calculate impurity\n",
        "                        impurity = self._impurity_calculation(y, y1, y2)\n",
        "\n",
        "                        # If this threshold resulted in a higher information gain than previously\n",
        "                        # recorded save the threshold value and the feature\n",
        "                        # index\n",
        "                        if impurity > largest_impurity:\n",
        "                            largest_impurity = impurity\n",
        "                            best_criteria = {\"feature_i\": feature_i, \"threshold\": threshold}\n",
        "                            best_sets = {\n",
        "                                \"leftX\": Xy1[:, :n_features],  # X of left subtree\n",
        "                                \"lefty\": Xy1[:, n_features:],  # y of left subtree\n",
        "                                \"rightX\": Xy2[:, :n_features],  # X of right subtree\n",
        "                                \"righty\": Xy2[:, n_features:]  # y of right subtree\n",
        "                            }\n",
        "\n",
        "        if largest_impurity > self.min_impurity:\n",
        "            # Build subtrees for the right and left branches\n",
        "            true_branch = self._build_tree(best_sets[\"leftX\"], best_sets[\"lefty\"], current_depth + 1)\n",
        "            false_branch = self._build_tree(best_sets[\"rightX\"], best_sets[\"righty\"], current_depth + 1)\n",
        "            return DecisionNode(feature_i=best_criteria[\"feature_i\"], threshold=best_criteria[\n",
        "                \"threshold\"], true_branch=true_branch, false_branch=false_branch)\n",
        "\n",
        "        # We're at leaf => determine value\n",
        "        leaf_value = self._leaf_value_calculation(y)\n",
        "        return DecisionNode(value=leaf_value)\n",
        "\n",
        "    def predict_value(self, x, tree=None):\n",
        "        \"\"\" Do a recursive search down the tree and make a prediction of the data sample by the\n",
        "            value of the leaf that we end up at \"\"\"\n",
        "\n",
        "        if tree is None:\n",
        "            tree = self.root\n",
        "\n",
        "        # If we have a value (i.e we're at a leaf) => return value as the prediction\n",
        "        if tree.value is not None:\n",
        "            return tree.value\n",
        "\n",
        "        # Choose the feature that we will test\n",
        "        feature_value = x[tree.feature_i]\n",
        "\n",
        "        # Determine if we will follow left or right branch\n",
        "        branch = tree.false_branch\n",
        "        if isinstance(feature_value, int) or isinstance(feature_value, float):\n",
        "            if feature_value >= tree.threshold:\n",
        "                branch = tree.true_branch\n",
        "        elif feature_value == tree.threshold:\n",
        "            branch = tree.true_branch\n",
        "\n",
        "        # Test subtree\n",
        "        return self.predict_value(x, branch)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\" Classify samples one by one and return the set of labels \"\"\"\n",
        "        y_pred = []\n",
        "        for x in X:\n",
        "            y_pred.append(self.predict_value(x))\n",
        "        return y_pred\n",
        "\n",
        "    def print_tree(self, tree=None, indent=\" \"):\n",
        "        \"\"\" Recursively print the decision tree \"\"\"\n",
        "        if not tree:\n",
        "            tree = self.root\n",
        "\n",
        "        # If we're at leaf => print the label\n",
        "        if tree.value is not None:\n",
        "            print(tree.value)\n",
        "        # Go deeper down the tree\n",
        "        else:\n",
        "            # Print test\n",
        "            print(\"%s:%s? \" % (tree.feature_i, tree.threshold))\n",
        "            # Print the true scenario\n",
        "            print(\"%sT->\" % (indent), end=\"\")\n",
        "            self.print_tree(tree.true_branch, indent + indent)\n",
        "            # Print the false scenario\n",
        "            print(\"%sF->\" % (indent), end=\"\")\n",
        "            self.print_tree(tree.false_branch, indent + indent)\n",
        "\n",
        "\n",
        "class ClassificationTree(DecisionTree):\n",
        "    def _calculate_information_gain(self, y, y1, y2):\n",
        "        # Calculate information gain\n",
        "        p = len(y1) / len(y)\n",
        "        entropy = calculate_entropy(y)\n",
        "        info_gain = entropy - p * \\\n",
        "                              calculate_entropy(y1) - (1 - p) * \\\n",
        "                                                      calculate_entropy(y2)\n",
        "        # print(\"info_gain\",info_gain)\n",
        "        return info_gain\n",
        "\n",
        "    def _majority_vote(self, y):\n",
        "        most_common = None\n",
        "        max_count = 0\n",
        "        for label in np.unique(y):\n",
        "            # Count number of occurences of samples with label\n",
        "            count = len(y[y == label])\n",
        "            if count > max_count:\n",
        "                most_common = label\n",
        "                max_count = count\n",
        "        # print(\"most_common :\",most_common)\n",
        "        return most_common\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self._impurity_calculation = self._calculate_information_gain\n",
        "        self._leaf_value_calculation = self._majority_vote\n",
        "        super(ClassificationTree, self).fit(X, y)\n",
        "\n",
        "\n",
        "class RegressionTree(DecisionTree):\n",
        "    def _calculate_variance_reduction(self, y, y1, y2):\n",
        "        var_tot = calculate_variance(y)\n",
        "        var_1 = calculate_variance(y1)\n",
        "        var_2 = calculate_variance(y2)\n",
        "        frac_1 = len(y1) / len(y)\n",
        "        frac_2 = len(y2) / len(y)\n",
        "\n",
        "        # Calculate the variance reduction\n",
        "        variance_reduction = var_tot - (frac_1 * var_1 + frac_2 * var_2)\n",
        "\n",
        "        return sum(variance_reduction)\n",
        "\n",
        "    def _mean_of_y(self, y):\n",
        "        value = np.mean(y, axis=0)\n",
        "        return value if len(value) > 1 else value[0]\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self._impurity_calculation = self._calculate_variance_reduction\n",
        "        self._leaf_value_calculation = self._mean_of_y\n",
        "        super(RegressionTree, self).fit(X, y)\n",
        "\n",
        "\n",
        "class XGBoostRegressionTree(DecisionTree):\n",
        "    \"\"\"\n",
        "    Regression tree for XGBoost\n",
        "    - Reference -\n",
        "    http://xgboost.readthedocs.io/en/latest/model.html\n",
        "    \"\"\"\n",
        "\n",
        "    def _split(self, y):\n",
        "        \"\"\" y contains y_true in left half of the middle column and\n",
        "        y_pred in the right half. Split and return the two matrices \"\"\"\n",
        "        col = int(np.shape(y)[1] / 2)\n",
        "        y, y_pred = y[:, :col], y[:, col:]\n",
        "        return y, y_pred\n",
        "\n",
        "    def _gain(self, y, y_pred):\n",
        "        nominator = np.power((y * self.loss.gradient(y, y_pred)).sum(), 2)\n",
        "        denominator = self.loss.hess(y, y_pred).sum()\n",
        "        return 0.5 * (nominator / denominator)\n",
        "\n",
        "    def _gain_by_taylor(self, y, y1, y2):\n",
        "        # Split\n",
        "        y, y_pred = self._split(y)\n",
        "        y1, y1_pred = self._split(y1)\n",
        "        y2, y2_pred = self._split(y2)\n",
        "\n",
        "        true_gain = self._gain(y1, y1_pred)\n",
        "        false_gain = self._gain(y2, y2_pred)\n",
        "        gain = self._gain(y, y_pred)\n",
        "        return true_gain + false_gain - gain\n",
        "\n",
        "    def _approximate_update(self, y):\n",
        "        # y split into y, y_pred\n",
        "        y, y_pred = self._split(y)\n",
        "        # Newton's Method\n",
        "        gradient = np.sum(y * self.loss.gradient(y, y_pred), axis=0)\n",
        "        hessian = np.sum(self.loss.hess(y, y_pred), axis=0)\n",
        "        update_approximation = gradient / hessian\n",
        "\n",
        "        return update_approximation\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self._impurity_calculation = self._gain_by_taylor\n",
        "        self._leaf_value_calculation = self._approximate_update\n",
        "        super(XGBoostRegressionTree, self).fit(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5fBDjNm3Mzp",
        "colab_type": "text"
      },
      "source": [
        "## GBDT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-fMrdlOUe6m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import division, print_function\n",
        "import numpy as np\n",
        "import progressbar\n",
        "\n",
        "# Import helper functions\n",
        "#from utils import train_test_split, standardize, to_categorical\n",
        "#from utils import mean_squared_error, accuracy_score\n",
        "#from utils.loss_functions import SquareLoss, CrossEntropy, SotfMaxLoss\n",
        "#from decision_tree.decision_tree_model import RegressionTree\n",
        "#from utils.misc import bar_widgets\n",
        "\n",
        "\n",
        "class GBDT(object):\n",
        "    \"\"\"Super class of GradientBoostingClassifier and GradientBoostinRegressor.\n",
        "    Uses a collection of regression trees that trains on predicting the gradient\n",
        "    of the loss function.\n",
        "    Parameters:\n",
        "    -----------\n",
        "    n_estimators: int\n",
        "        树的数量\n",
        "        The number of classification trees that are used.\n",
        "    learning_rate: float\n",
        "        梯度下降的学习率\n",
        "        The step length that will be taken when following the negative gradient during\n",
        "        training.\n",
        "    min_samples_split: int\n",
        "        每棵子树的节点的最小数目（小于后不继续切割）\n",
        "        The minimum number of samples needed to make a split when building a tree.\n",
        "    min_impurity: float\n",
        "        每颗子树的最小纯度（小于后不继续切割）\n",
        "        The minimum impurity required to split the tree further.\n",
        "    max_depth: int\n",
        "        每颗子树的最大层数（大于后不继续切割）\n",
        "        The maximum depth of a tree.\n",
        "    regression: boolean\n",
        "        是否为回归问题\n",
        "        True or false depending on if we're doing regression or classification.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_estimators, learning_rate, min_samples_split,\n",
        "                 min_impurity, max_depth, regression):\n",
        "\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_impurity = min_impurity\n",
        "        self.max_depth = max_depth\n",
        "        self.regression = regression\n",
        "\n",
        "        # 进度条 processbar\n",
        "        self.bar = progressbar.ProgressBar(widgets=bar_widgets)\n",
        "\n",
        "        self.loss = SquareLoss()\n",
        "        if not self.regression:\n",
        "            self.loss = SotfMaxLoss()\n",
        "\n",
        "        # 分类问题也使用回归树，利用残差去学习概率\n",
        "        self.trees = []\n",
        "        for i in range(self.n_estimators):\n",
        "            self.trees.append(RegressionTree(min_samples_split=self.min_samples_split,\n",
        "                                             min_impurity=self.min_impurity,\n",
        "                                             max_depth=self.max_depth))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # 让第一棵树去拟合模型\n",
        "        self.trees[0].fit(X, y)\n",
        "        y_pred = self.trees[0].predict(X)\n",
        "        for i in self.bar(range(1, self.n_estimators)):\n",
        "            gradient = self.loss.gradient(y, y_pred)\n",
        "            self.trees[i].fit(X, gradient)\n",
        "            y_pred -= np.multiply(self.learning_rate, self.trees[i].predict(X))\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = self.trees[0].predict(X)\n",
        "        for i in range(1, self.n_estimators):\n",
        "            y_pred -= np.multiply(self.learning_rate, self.trees[i].predict(X))\n",
        "\n",
        "        if not self.regression:\n",
        "            # Turn into probability distribution\n",
        "            y_pred = np.exp(y_pred) / np.expand_dims(np.sum(np.exp(y_pred), axis=1), axis=1)\n",
        "            # Set label to the value that maximizes probability\n",
        "            y_pred = np.argmax(y_pred, axis=1)\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "class GBDTRegressor(GBDT):\n",
        "    def __init__(self, n_estimators=200, learning_rate=0.5, min_samples_split=2,\n",
        "                 min_var_red=1e-7, max_depth=4, debug=False):\n",
        "        super(GBDTRegressor, self).__init__(n_estimators=n_estimators,\n",
        "                                            learning_rate=learning_rate,\n",
        "                                            min_samples_split=min_samples_split,\n",
        "                                            min_impurity=min_var_red,\n",
        "                                            max_depth=max_depth,\n",
        "                                            regression=True)\n",
        "\n",
        "\n",
        "class GBDTClassifier(GBDT):\n",
        "    def __init__(self, n_estimators=200, learning_rate=.5, min_samples_split=2,\n",
        "                 min_info_gain=1e-7, max_depth=2, debug=False):\n",
        "        super(GBDTClassifier, self).__init__(n_estimators=n_estimators,\n",
        "                                             learning_rate=learning_rate,\n",
        "                                             min_samples_split=min_samples_split,\n",
        "                                             min_impurity=min_info_gain,\n",
        "                                             max_depth=max_depth,\n",
        "                                             regression=False)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        y = to_categorical(y)\n",
        "        super(GBDTClassifier, self).fit(X, y)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbovQ6OK3SS1",
        "colab_type": "text"
      },
      "source": [
        "## 测试"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqSHlKP7Ue23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!wget https://github.com/xiaochengJF/Machine-Learning-From-Scratch/blob/master/TempLinkoping2016.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hpTJZvvXEWw",
        "colab_type": "code",
        "outputId": "48f112e6-70e5-445f-e4c4-93ea686ae234",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "cd /content/drive/My Drive/MachineLearning/MLCode/Projects/Machine-Learning-From-Scratch/gradient_boosting_decision_tree"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/MachineLearning/MLCode/Projects/Machine-Learning-From-Scratch/gradient_boosting_decision_tree\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTgb9hBfUCGa",
        "colab_type": "code",
        "outputId": "6caeb83b-9214-4b09-82b8-12fef51e5406",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        }
      },
      "source": [
        "from __future__ import division, print_function\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import progressbar\n",
        "\n",
        "#from utils import train_test_split, standardize, to_categorical\n",
        "#from utils import mean_squared_error, accuracy_score, Plot\n",
        "#from utils.misc import bar_widgets\n",
        "#from gradient_boosting_decision_tree.gbdt_model import GBDTRegressor\n",
        "\n",
        "\n",
        "def main():\n",
        "    print (\"-- Gradient Boosting Regression --\")\n",
        "\n",
        "    # Load temperature data\n",
        "    data = pd.read_csv('../TempLinkoping2016.txt', sep=\"\\t\")\n",
        "    #data = pd.read_csv('TempLinkoping2016.txt', sep=\"\\t\")\n",
        "\n",
        "    time = np.atleast_2d(data[\"time\"].as_matrix()).T\n",
        "    temp = np.atleast_2d(data[\"temp\"].as_matrix()).T\n",
        "\n",
        "    X = time.reshape((-1, 1))               # Time. Fraction of the year [0, 1]\n",
        "    X = np.insert(X, 0, values=1, axis=1)   # Insert bias term\n",
        "    y = temp[:, 0]                          # Temperature. Reduce to one-dim\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n",
        "\n",
        "    model = GBDTRegressor()\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    y_pred_line = model.predict(X)\n",
        "\n",
        "    # Color map\n",
        "    cmap = plt.get_cmap('viridis')\n",
        "\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "    print (\"Mean Squared Error:\", mse)\n",
        "\n",
        "    # Plot the results\n",
        "    m1 = plt.scatter(366 * X_train[:, 1], y_train, color=cmap(0.9), s=10)\n",
        "    m2 = plt.scatter(366 * X_test[:, 1], y_test, color=cmap(0.5), s=10)\n",
        "    m3 = plt.scatter(366 * X_test[:, 1], y_pred, color='black', s=10)\n",
        "    plt.suptitle(\"Regression Tree\")\n",
        "    plt.title(\"MSE: %.2f\" % mse, fontsize=10)\n",
        "    plt.xlabel('Day')\n",
        "    plt.ylabel('Temperature in Celcius')\n",
        "    plt.legend((m1, m2, m3), (\"Training data\", \"Test data\", \"Prediction\"), loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-- Gradient Boosting Regression --\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:20: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:21: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "Training: 100% [-----------------------------------------------] Time:  0:00:25\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Mean Squared Error: 10.49341738752049\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TT5E3k1XJ4B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}